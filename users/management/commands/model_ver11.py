# model_ver11.py

# ------------------------------------------------------------
# KoELECTRA ë©€í‹°ë¼ë²¨ í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ (ë¹µì§‘/ì¹´í˜ ë¦¬ë·°ìš©)
# - ver10 ê°œì„  + ë¶€ì • ë¬¸ë§¥(í•˜ì§€ ì•Šë‹¤/ì—†ë‹¤ ë“±) ì²˜ë¦¬ ê°•í™”
# - í‚¤ì›Œë“œë³„ "ì–‘ì„± ë“±ì¥(review ë¹„ìœ¨/íšŒìˆ˜)"ê¹Œì§€ ì‚°ì¶œí•´ JSONì— ì €ì¥
# - ê³¼ë„í•œ ìµœì†Œ ë“±ì¥ íšŸìˆ˜ í•„í„° ëŒ€ì‹ , ë¹ˆë„/ë¹„ìœ¨ì€ ë‚˜ì¤‘ ë­í‚¹ ë‹¨ê³„ì—ì„œ í™œìš©
# ------------------------------------------------------------

import os
import re
import json
import math
import random
from typing import List, Dict, Any, Tuple, Optional

import numpy as np
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F

from transformers import AutoTokenizer, AutoModel

# ============================================================
# 0. ê¸°ë³¸ ì„¤ì •
# ============================================================

BASE_MODEL_NAME = "monologg/koelectra-base-v3-discriminator"

# ì´ ê°’ë“¤ì€ "íŒŒì¼ ì´ë¦„"ë§Œ ì •ì˜í•©ë‹ˆë‹¤. ì‹¤ì œ ê²½ë¡œëŠ” ì‚¬ìš©í•  ë•Œ ì¡°í•©í•©ë‹ˆë‹¤.
BASE_KEYWORD_PATH = "base_keywords.json"
NEW_KEYWORD_PATH = "new_keyword.json"
TRAIN_DIR = "train_ver2"  # í•™ìŠµ ì‹œì—ë§Œ ì‚¬ìš©
CHECKPOINT_PATH = "best_koelectra_model_ver11.pth"  # â˜… ver11 ì €ì¥ ì´ë¦„

DESSERT_META_PATH = "dessert_en.json"

MAX_SEQ_LEN = 256
BATCH_SIZE = 16
EPOCHS = 3
K_FOLD = 1  # 1ì´ë©´ train/val 8:2, ì´í›„ full train

BASE_LR = 3e-5
HEAD_LR = 1e-4

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"DEVICE: {DEVICE}")


def normalize_text_basic(text: str) -> str:
    return text.replace(" ", "").lower()


# ============================================================
# 1. ì‹œë“œ & JSON ìœ í‹¸
# ============================================================

def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def load_json(path: str) -> Any:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def save_json(data: Any, path: str):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


# ============================================================
# 2. í‚¤ì›Œë“œ ì„¤ì •
# ============================================================

def load_keyword_config(
    base_path: str = BASE_KEYWORD_PATH,
    new_path: str = NEW_KEYWORD_PATH,
) -> Tuple[Dict[str, List[str]], List[str], Dict[str, int], Dict[int, str], Dict[str, str]]:
    """
    base_keywords.json + new_keyword.json ì„ í•©ì³ ìµœì¢… ë¼ë²¨ ë¦¬ìŠ¤íŠ¸ì™€
    ì¹´í…Œê³ ë¦¬ ë§µ(kw2cat)ì„ ë§Œë“ ë‹¤.
    """
    base_cfg = load_json(base_path)
    categories = ["menu", "taste", "texture", "topping", "store"]

    kw_by_cat: Dict[str, List[str]] = {}
    for cat in categories:
        kw_by_cat[cat] = base_cfg.get(cat, [])

    base_all_raw = base_cfg.get("all_keywords", [])
    base_all = sorted(list(set(base_all_raw)))

    # new_keyword.json ì´ ìˆìœ¼ë©´ ì¶”ê°€ í‚¤ì›Œë“œ ë¡œë“œ
    if os.path.exists(new_path):
        new_cfg = load_json(new_path)
        new_all_raw = new_cfg.get("all_keywords", [])
    else:
        new_all_raw = []

    new_only = [k for k in new_all_raw if k not in base_all]
    all_keywords = base_all + new_only

    label2id = {kw: i for i, kw in enumerate(all_keywords)}
    id2label = {i: kw for kw, i in label2id.items()}

    # kw2cat: ê° í‚¤ì›Œë“œ â†’ ì¹´í…Œê³ ë¦¬
    kw2cat: Dict[str, str] = {}
    for cat, kws in kw_by_cat.items():
        for k in kws:
            kw2cat[k] = cat

    # new_only ì¤‘ ì¹´í…Œê³ ë¦¬ ì§€ì • ì•ˆ ëœ ê²ƒì€ ì¼ë‹¨ menu ë¡œ
    for k in new_only:
        if k not in kw2cat:
            kw2cat[k] = "menu"

    print("------------------------------------------------------------")
    print("ğŸ“‹ í‚¤ì›Œë“œ ì„¤ì • ìš”ì•½")
    print(f"  - base_all_raw ê°œìˆ˜ (ì¤‘ë³µ í¬í•¨): {len(base_all_raw)}")
    print(f"  - base_all ê°œìˆ˜ (ì¤‘ë³µ ì œê±°): {len(base_all)}")
    print(f"  - new_keywords_raw ê°œìˆ˜: {len(new_all_raw)}")
    print(f"  - new_only ê°œìˆ˜: {len(new_only)}")
    print(f"  - ìµœì¢… ë¼ë²¨ ê°œìˆ˜: {len(all_keywords)}")
    print("------------------------------------------------------------")

    return kw_by_cat, all_keywords, label2id, id2label, kw2cat


# ============================================================
# 3. dessert_en ê¸°ë°˜ ë§¤ì¥ ë©”íƒ€ (ìŒë£Œ/ë¹µ ë¹„ìœ¨ë§Œ ì‚¬ìš©)
# ============================================================

BEVERAGE_TOKENS = [
    "ì»¤í”¼", "ì•„ë©”ë¦¬ì¹´ë…¸", "ë¼ë–¼", "ë¼í…Œ", "ì½œë“œë¸Œë£¨",
    "ì¹´í‘¸ì¹˜ë…¸", "ì—ìŠ¤í”„ë ˆì†Œ", "ìŒë£Œ", "ì°¨", "í‹°",
    "ì—ì´ë“œ", "ìŠ¤ë¬´ë””", "ì£¼ìŠ¤"
]

DESSERT_TOKENS = [
    "ë¹µ", "ë””ì €íŠ¸", "ì¼€ì´í¬", "ë² ì´ì»¤ë¦¬",
    "í¬ë¡œì™€ìƒ", "í¬ë£¨ì•„ìƒ", "ìŠ¤ì½˜", "ì¿ í‚¤", "íƒ€ë¥´íŠ¸",
    "ë§ˆì¹´ë¡±", "ë§ˆë“¤ë Œ", "íœ˜ë‚­ì‹œì—", "ê¹ŒëˆŒë ˆ", "ë„ë„›", "ë„ë„ˆì¸ ",
    "í¬ë¡œí”Œ", "ë¸Œë¼ìš°ë‹ˆ", "íŒŒì´", "ë¡¤ì¼€ì´í¬", "ìƒŒë“œìœ„ì¹˜"
]


def build_place_profile_from_meta(meta: Dict[str, Any]) -> Dict[str, Any]:
    """
    dessert_en.json ì•ˆì˜ review_keywords ê¸°ë°˜ìœ¼ë¡œ
    'ìŒë£Œ ìœ„ì£¼ ì¹´í˜'ì¸ì§€ ëŒ€ëµ íŒë³„.
    """
    review_keywords = meta.get("review_keywords", [])

    beverage_score = 0
    dessert_score = 0

    for item in review_keywords:
        kw_text = item.get("keyword", "")
        cnt = item.get("count", 0)
        if any(tok in kw_text for tok in BEVERAGE_TOKENS):
            beverage_score += cnt
        if any(tok in kw_text for tok in DESSERT_TOKENS):
            dessert_score += cnt

    menu_keywords = set()

    is_beverage_shop = False
    if beverage_score >= 20 and beverage_score >= 2 * max(dessert_score, 1):
        is_beverage_shop = True

    return {
        "is_beverage_shop": is_beverage_shop,
        "menu_keywords": menu_keywords,
        "beverage_score": beverage_score,
        "dessert_score": dessert_score,
    }


def load_dessert_profiles(path: str = DESSERT_META_PATH) -> Dict[str, Dict[str, Any]]:
    if not os.path.exists(path):
        print(f"âš ï¸ dessert_en.json ì—†ìŒ: {path} (ë§¤ì¥ ë©”íƒ€ ì—†ì´ ì§„í–‰)")
        return {}

    raw = load_json(path)
    profiles: Dict[str, Dict[str, Any]] = {}

    if isinstance(raw, list):
        items = raw
    elif isinstance(raw, dict):
        items = list(raw.values())
    else:
        items = []

    for item in items:
        if not isinstance(item, dict):
            continue
        name = item.get("name") or item.get("place_name")
        if not name:
            continue
        profiles[name] = build_place_profile_from_meta(item)

    print(f"ğŸ“¦ dessert_en ê¸°ë°˜ ë§¤ì¥ ë©”íƒ€ í”„ë¡œí•„ ìˆ˜: {len(profiles)}")
    return profiles


def get_place_profile(
    place_name: str,
    profiles: Dict[str, Dict[str, Any]],
) -> Dict[str, Any]:
    if place_name in profiles:
        return profiles[place_name]

    return {
        "is_beverage_shop": False,
        "menu_keywords": set(),
        "beverage_score": 0,
        "dessert_score": 0,
    }


# ============================================================
# 4. ê·œì¹™ ê¸°ë°˜ ì¶”ì¶œ + ë¶€ì • ë¬¸ë§¥ ì²˜ë¦¬
# ============================================================

def normalize_text_for_rule(text: str) -> str:
    # ê³µë°± ì œê±° + ì†Œë¬¸ì
    return re.sub(r"\s+", "", text).lower()


def is_negated_keyword(kw_norm: str, text_norm: str) -> bool:
    """
    kw_norm: ê³µë°±/ëŒ€ì†Œë¬¸ì ì œê±°ëœ í‚¤ì›Œë“œ (ì˜ˆ: 'ë°”ì‚­ë°”ì‚­', 'ì›¨ì´íŒ…')
    text_norm: ê³µë°± ì œê±°ëœ ë¦¬ë·° ì „ì²´ í…ìŠ¤íŠ¸
    - 'ë°”ì‚­ë°”ì‚­í•˜ì§€ì•Šë‹¤', 'ì•ˆë°”ì‚­ë°”ì‚­í•¨', 'ì›¨ì´íŒ…ì—†ë‹¤', 'ì›¨ì´íŒ…ì•ˆí•˜ê³ ' ë“±
      ë¶€ì •/ë¶€ì •ì  ìƒí™©ì´ë©´ Trueë¥¼ ë°˜í™˜.
    """
    # í‚¤ì›Œë“œ ì£¼ë³€ 8ê¸€ì ì •ë„ ìœˆë„ìš° ì•ˆì—ì„œ ë¶€ì • íŒ¨í„´ë§Œ í™•ì¸
    idx = 0
    while True:
        idx = text_norm.find(kw_norm, idx)
        if idx == -1:
            break

        start = max(0, idx - 8)
        end = min(len(text_norm), idx + len(kw_norm) + 8)
        window = text_norm[start:end]

        # 1) "í‚¤ì›Œë“œ+í•˜ì§€ì•Š/ì§€ì•Š/ì—†ë‹¤/ì—†ìŒ/ì—†ì–´ì„œ" íŒ¨í„´
        if re.search(
            kw_norm + r"(í•˜ì§€ì•Š|ì§€ì•Š|ì§€ì•Šê³ |ì§€ì•Šì•˜|ì—†ë‹¤|ì—†ê³ |ì—†ì–´ì„œ|ì—†ì–´|ì—†ìŒ)",
            window
        ):
            return True

        # 2) "ì•ˆ/ëª»/ë³„ë¡œ/ì „í˜€/ê·¸ë‹¥/ê·¸ë‹¤ì§€ + í‚¤ì›Œë“œ" íŒ¨í„´
        if re.search(
            r"(ì•ˆ|ëª»|ë³„ë¡œ|ì „í˜€|ê·¸ë‹¥|ê·¸ë‹¤ì§€)[^ê°€-í£0-9]*" + kw_norm,
            window
        ):
            return True

        # 3) "í‚¤ì›Œë“œ + ëŠë‚Œì´ì—†ë‹¤/ëŠë‚Œì•ˆë‚œë‹¤" í™•ì¥ íŒ¨í„´
        if re.search(
            kw_norm + r"[^ê°€-í£0-9]*(ëŠë‚Œì´ì—†|ëŠë‚Œì•ˆë‚˜|ëŠë‚Œì´ì•ˆë‚˜)",
            window
        ):
            return True

        idx += len(kw_norm)

    return False


def has_positive_occurrence(kw: str, text_norm: str) -> bool:
    """
    ë¦¬ë·° í…ìŠ¤íŠ¸(ê³µë°± ì œê±°) ì•ˆì—ì„œ:
      - í•´ë‹¹ í‚¤ì›Œë“œê°€ ë“±ì¥í•˜ê³ 
      - ë¶€ì • ë¬¸ë§¥ì´ ì•„ë‹ˆë¼ë©´
    Trueë¥¼ ë°˜í™˜.
    """
    kw_norm = kw.replace(" ", "").lower()
    if not kw_norm:
        return False

    if kw_norm not in text_norm:
        return False

    if is_negated_keyword(kw_norm, text_norm):
        return False

    return True


def rule_based_extract_keywords(
    text: str,
    kw_by_cat: Dict[str, List[str]],
) -> Dict[str, List[str]]:
    """
    ê·œì¹™ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ:
    - í…ìŠ¤íŠ¸ì— í‚¤ì›Œë“œê°€ ì‹¤ì œ ë“±ì¥í•˜ê³ 
    - ë¶€ì • ë¬¸ë§¥ì´ ì•„ë‹Œ ê²½ìš°ë§Œ hit ë¡œ ì¸ì •
    """
    norm = normalize_text_for_rule(text)
    hits = {
        "menu": [],
        "taste": [],
        "texture": [],
        "topping": [],
        "store": [],
    }

    for cat, kw_list in kw_by_cat.items():
        for kw in kw_list:
            if has_positive_occurrence(kw, norm):
                hits[cat].append(kw)

    for cat in hits:
        hits[cat] = sorted(list(set(hits[cat])))
    return hits


def detect_kw_surface_in_reviews(
    reviews: List[Dict[str, Any]],
    candidate_keywords: List[str],
) -> Dict[str, bool]:
    """
    place_final_keywords ì¤‘ì—ì„œ
    - ì‹¤ì œ ë¦¬ë·° í…ìŠ¤íŠ¸ì—ì„œ
    - 'ê¸ì •/ì¤‘ë¦½ ë¬¸ë§¥'ìœ¼ë¡œ í•œ ë²ˆ ì´ìƒ ë“±ì¥í•œ í‚¤ì›Œë“œë§Œ True.
    """
    norm_reviews = []
    for rv in reviews:
        txt = rv.get("review_content", "")
        if not txt:
            continue
        norm_reviews.append(normalize_text_for_rule(txt))

    seen = {kw: False for kw in candidate_keywords}
    for kw in candidate_keywords:
        if not kw:
            continue
        kw_norm = kw.replace(" ", "").lower()
        if not kw_norm:
            continue

        for ntxt in norm_reviews:
            if has_positive_occurrence(kw, ntxt):
                seen[kw] = True
                break
    return seen


def make_review_text_concat(reviews: List[Dict[str, Any]]) -> str:
    texts = [r.get("review_content", "") for r in reviews if r.get("review_content", "")]
    return "\n".join(texts)


# ============================================================
# 5. í•™ìŠµ ë°ì´í„° ë¡œë”© (JSON í•™ìŠµìš© â€“ ì§€ê¸ˆì€ DB ì¶”ë¡ ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
# ============================================================

def load_train_data(
    train_dir: str,
    label2id: Dict[str, int],
    kw_by_cat: Dict[str, List[str]],
    place_profiles: Dict[str, Dict[str, Any]],
) -> List[Dict[str, Any]]:
    """
    train_ver2 ì•ˆì˜ ê° JSON (ë§¤ì¥ ë‹¨ìœ„ ë°ì´í„°)ì„ ì½ì–´
    ë¦¬ë·° ë‹¨ìœ„ í•™ìŠµ ìƒ˜í”Œì„ êµ¬ì„±í•œë‹¤.
    """
    samples: List[Dict[str, Any]] = []
    files = [f for f in os.listdir(train_dir) if f.endswith(".json")]
    print(f"ğŸ“‚ í•™ìŠµ íŒŒì¼ ê°œìˆ˜: {len(files)}")

    for fname in files:
        path = os.path.join(train_dir, fname)
        data = load_json(path)
        place = data.get("place_name", fname)

        profile = get_place_profile(place, place_profiles)
        is_beverage_shop = profile["is_beverage_shop"]

        kw_obj = data.get("keywords", {})
        place_final_keywords = kw_obj.get("final_keywords", [])
        place_final_keywords = [k for k in place_final_keywords if k in label2id]

        reviews = data.get("reviews", [])
        if not reviews:
            continue

        # place_final_keywords ì¤‘ ì‹¤ì œ ë¦¬ë·°ì—ì„œ "ê¸ì • ë¬¸ë§¥"ìœ¼ë¡œ ë“±ì¥í•œ ê²ƒë§Œ ë‚¨ê¸°ê¸°
        seen_map = detect_kw_surface_in_reviews(reviews, place_final_keywords)
        filtered_final_keywords = [k for k in place_final_keywords if seen_map.get(k, False)]
        place_final_keywords = filtered_final_keywords

        if not place_final_keywords:
            continue

        for rv in reviews:
            text = rv.get("review_content", "").strip()
            if not text:
                continue

            # ê°œë³„ ë¦¬ë·° ìˆ˜ì¤€ rule-based hit (ë¶€ì • ë¬¸ë§¥ ì œê±° ë°˜ì˜ë¨)
            rule_hits = rule_based_extract_keywords(text, kw_by_cat)
            rule_final = []
            for cat in ["menu", "taste", "texture", "topping", "store"]:
                rule_final.extend(rule_hits.get(cat, []))

            merged_labels = set(place_final_keywords) | set(rule_final)
            merged_labels = [k for k in merged_labels if k in label2id]

            if not merged_labels:
                continue

            label_vec = np.zeros(len(label2id), dtype=np.float32)
            for k in merged_labels:
                label_vec[label2id[k]] = 1.0

            samples.append(
                {
                    "text": text,
                    "labels": label_vec,
                    "place_name": place,
                    "is_beverage_shop": is_beverage_shop,
                }
            )

    print(f"âœ… ìƒì„±ëœ í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(samples)}")
    return samples


# ============================================================
# 6. Dataset / Dataloader
# ============================================================

class ReviewDataset(Dataset):
    def __init__(
        self,
        samples: List[Dict[str, Any]],
        tokenizer: AutoTokenizer,
        max_len: int,
    ):
        self.samples = samples
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx: int):
        item = self.samples[idx]
        text = item["text"]
        labels = item["labels"]

        enc = self.tokenizer(
            text,
            padding="max_length",
            truncation=True,
            max_length=self.max_len,
            return_tensors="pt",
        )

        return {
            "input_ids": enc["input_ids"].squeeze(0),
            "attention_mask": enc["attention_mask"].squeeze(0),
            "labels": torch.tensor(labels, dtype=torch.float32),
        }


def create_dataloader(dataset: Dataset, batch_size: int, shuffle: bool) -> DataLoader:
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=0)


# ============================================================
# 7. ëª¨ë¸ ì •ì˜
# ============================================================

class KeywordExtractorModel(nn.Module):
    def __init__(self, num_labels: int):
        super().__init__()
        self.encoder = AutoModel.from_pretrained(BASE_MODEL_NAME)
        hidden_size = self.encoder.config.hidden_size
        self.classifier = nn.Linear(hidden_size, num_labels)

    def forward(self, input_ids, attention_mask):
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        pooled = outputs.last_hidden_state[:, 0, :]  # [CLS] í† í°
        logits = self.classifier(pooled)
        return logits


# ============================================================
# 8. pos_weight + ì¹´í…Œê³ ë¦¬ë³„ loss ê°€ì¤‘ì¹˜
# ============================================================

def compute_pos_weight(samples: List[Dict[str, Any]], num_labels: int) -> torch.Tensor:
    label_sum = np.zeros(num_labels, dtype=np.float64)
    for s in samples:
        label_sum += s["labels"]

    n_samples = len(samples)
    pos = label_sum
    neg = n_samples - pos

    pos = np.clip(pos, 1.0, None)
    neg = np.clip(neg, 1.0, None)

    pos_weight = neg / pos
    pos_weight = np.clip(pos_weight, 1.0, 5.0)

    print("ğŸ“Š pos_weight í†µê³„")
    print(f"  - min: {pos_weight.min():.3f}")
    print(f"  - max: {pos_weight.max():.3f}")
    print(f"  - mean: {pos_weight.mean():.3f}")
    return torch.tensor(pos_weight, dtype=torch.float32)


def build_label_loss_weights(
    label_list: List[str],
    kw2cat: Dict[str, str],
) -> torch.Tensor:
    """
    taste/texture/topping ì€ loss ê°€ì¤‘ì¹˜ë¥¼ ì¡°ê¸ˆ ë” ë†’ê²Œ,
    store ëŠ” ì‚´ì§ ë†’ê²Œ, menu ëŠ” ê¸°ë³¸ê°’ 1.0
    """
    w = np.ones(len(label_list), dtype=np.float32)
    for i, kw in enumerate(label_list):
        cat = kw2cat.get(kw, "menu")
        if cat in ["taste", "texture", "topping"]:
            w[i] = 1.5
        elif cat == "store":
            w[i] = 1.2
        else:
            w[i] = 1.0

    print("ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ label_loss_weights ì„¤ì • ì™„ë£Œ")
    return torch.tensor(w, dtype=torch.float32)


# ============================================================
# 9. K-Fold ìœ í‹¸
# ============================================================

def make_kfold_indices(n_samples: int, k: int, seed: int = 42):
    indices = list(range(n_samples))
    rng = random.Random(seed)
    rng.shuffle(indices)

    fold_sizes = [n_samples // k] * k
    for i in range(n_samples % k):
        fold_sizes[i] += 1

    current = 0
    folds = []
    for fs in fold_sizes:
        start, stop = current, current + fs
        folds.append(indices[start:stop])
        current = stop

    for i in range(k):
        val_idx = folds[i]
        train_idx = [idx for j, f in enumerate(folds) if j != i for idx in f]
        yield train_idx, val_idx


# ============================================================
# 10. í•™ìŠµ ë£¨í”„
# ============================================================

def train_one_fold(
    fold_id: int,
    train_samples: List[Dict[str, Any]],
    val_samples: List[Dict[str, Any]],
    label_list: List[str],
    kw2cat: Dict[str, str],
    pos_weight: torch.Tensor,
):
    print("------------------------------------------------------------")
    print(f"ğŸ“‚ Fold {fold_id} í•™ìŠµ ì‹œì‘ (train {len(train_samples)}, val {len(val_samples)})")

    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)
    train_dataset = ReviewDataset(train_samples, tokenizer, MAX_SEQ_LEN)
    val_dataset = ReviewDataset(val_samples, tokenizer, MAX_SEQ_LEN)

    train_loader = create_dataloader(train_dataset, BATCH_SIZE, True)
    val_loader = create_dataloader(val_dataset, BATCH_SIZE, False)

    model = KeywordExtractorModel(num_labels=len(label_list)).to(DEVICE)

    label_loss_weights = build_label_loss_weights(label_list, kw2cat).to(DEVICE)
    pos_weight = pos_weight.to(DEVICE)

    no_decay = ["bias", "LayerNorm.weight"]
    encoder_params, classifier_params = [], []

    for name, param in model.named_parameters():
        if "classifier" in name:
            classifier_params.append((name, param))
        else:
            encoder_params.append((name, param))

    optimizer_grouped_parameters = [
        {
            "params": [p for n, p in encoder_params if not any(nd in n for nd in no_decay)],
            "weight_decay": 0.01,
            "lr": BASE_LR,
        },
        {
            "params": [p for n, p in encoder_params if any(nd in n for nd in no_decay)],
            "weight_decay": 0.0,
            "lr": BASE_LR,
        },
        {
            "params": [p for n, p in classifier_params if not any(nd in n for nd in no_decay)],
            "weight_decay": 0.01,
            "lr": HEAD_LR,
        },
        {
            "params": [p for n, p in classifier_params if any(nd in n for nd in no_decay)],
            "weight_decay": 0.0,
            "lr": HEAD_LR,
        },
    ]
    optimizer = torch.optim.AdamW(optimizer_grouped_parameters)

    def bce_with_pos_and_cat(logits, labels):
        loss_per_label = F.binary_cross_entropy_with_logits(
            logits,
            labels,
            pos_weight=pos_weight,
            reduction="none",
        )
        loss_per_label = loss_per_label * label_loss_weights
        return loss_per_label.mean()

    best_val = float("inf")
    for epoch in range(1, EPOCHS + 1):
        model.train()
        tr_loss, tr_steps = 0.0, 0

        for batch in train_loader:
            optimizer.zero_grad()
            input_ids = batch["input_ids"].to(DEVICE)
            attention_mask = batch["attention_mask"].to(DEVICE)
            labels = batch["labels"].to(DEVICE)

            logits = model(input_ids, attention_mask)
            loss = bce_with_pos_and_cat(logits, labels)

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            tr_loss += loss.item()
            tr_steps += 1

        tr_loss /= max(tr_steps, 1)

        model.eval()
        val_loss, val_steps = 0.0, 0
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch["input_ids"].to(DEVICE)
                attention_mask = batch["attention_mask"].to(DEVICE)
                labels = batch["labels"].to(DEVICE)

                logits = model(input_ids, attention_mask)
                loss = bce_with_pos_and_cat(logits, labels)

                val_loss += loss.item()
                val_steps += 1

        val_loss /= max(val_steps, 1)
        print(f"[Fold {fold_id}] Epoch {epoch}/{EPOCHS} - train_loss: {tr_loss:.4f}, val_loss: {val_loss:.4f}")

        if val_loss < best_val:
            best_val = val_loss

    return best_val


def run_kfold_training(
    samples: List[Dict[str, Any]],
    label_list: List[str],
    kw2cat: Dict[str, str],
):
    pos_weight = compute_pos_weight(samples, len(label_list))
    n_samples = len(samples)
    fold_losses = []

    if K_FOLD <= 1:
        print("============================================================")
        print("ğŸš€ ë‹¨ì¼ train/val í•™ìŠµ ì‹œì‘ (K_FOLD=1)")
        print("============================================================")

        idx = list(range(n_samples))
        random.shuffle(idx)
        split = int(n_samples * 0.8)
        train_idx, val_idx = idx[:split], idx[split:]
        train_samples = [samples[i] for i in train_idx]
        val_samples = [samples[i] for i in val_idx]

        val_loss = train_one_fold(1, train_samples, val_samples, label_list, kw2cat, pos_weight)
        fold_losses.append(val_loss)
    else:
        print("============================================================")
        print(f"ğŸš€ K-Fold í•™ìŠµ ì‹œì‘ (K={K_FOLD})")
        print("============================================================")
        for fold_id, (train_idx, val_idx) in enumerate(
            make_kfold_indices(n_samples, K_FOLD, seed=42), start=1
        ):
            train_samples = [samples[i] for i in train_idx]
            val_samples = [samples[i] for i in val_idx]
            val_loss = train_one_fold(fold_id, train_samples, val_samples, label_list, kw2cat, pos_weight)
            fold_losses.append(val_loss)

    print("============================================================")
    print("âœ… K-Fold/ë‹¨ì¼ í•™ìŠµ ì¢…ë£Œ")
    for i, l in enumerate(fold_losses, start=1):
        print(f"  - Fold {i} val_loss: {l:.4f}")
    print(f"  - í‰ê·  val_loss: {np.mean(fold_losses):.4f}")
    print("============================================================")

    # ---------------------------------------------------------
    # ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ
    # ---------------------------------------------------------
    print("ğŸ“¦ ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ ì‹œì‘")

    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)
    full_dataset = ReviewDataset(samples, tokenizer, MAX_SEQ_LEN)
    full_loader = create_dataloader(full_dataset, BATCH_SIZE, True)

    model = KeywordExtractorModel(num_labels=len(label_list)).to(DEVICE)
    label_loss_weights = build_label_loss_weights(label_list, kw2cat).to(DEVICE)
    pos_weight_tensor = compute_pos_weight(samples, len(label_list)).to(DEVICE)

    no_decay = ["bias", "LayerNorm.weight"]
    encoder_params, classifier_params = [], []
    for name, param in model.named_parameters():
        if "classifier" in name:
            classifier_params.append((name, param))
        else:
            encoder_params.append((name, param))

    optimizer_grouped_parameters = [
        {
            "params": [p for n, p in encoder_params if not any(nd in n for nd in no_decay)],
            "weight_decay": 0.01,
            "lr": BASE_LR,
        },
        {
            "params": [p for n, p in encoder_params if any(nd in n for nd in no_decay)],
            "weight_decay": 0.0,
            "lr": BASE_LR,
        },
        {
            "params": [p for n, p in classifier_params if not any(nd in n for nd in no_decay)],
            "weight_decay": 0.01,
            "lr": HEAD_LR,
        },
        {
            "params": [p for n, p in classifier_params if any(nd in n for nd in no_decay)],
            "weight_decay": 0.0,
            "lr": HEAD_LR,
        },
    ]
    optimizer = torch.optim.AdamW(optimizer_grouped_parameters)

    def bce_full(logits, labels):
        loss_per_label = F.binary_cross_entropy_with_logits(
            logits,
            labels,
            pos_weight=pos_weight_tensor,
            reduction="none",
        )
        loss_per_label = loss_per_label * label_loss_weights
        return loss_per_label.mean()

    for epoch in range(1, EPOCHS + 1):
        model.train()
        total_loss, steps = 0.0, 0
        for batch in full_loader:
            optimizer.zero_grad()
            input_ids = batch["input_ids"].to(DEVICE)
            attention_mask = batch["attention_mask"].to(DEVICE)
            labels = batch["labels"].to(DEVICE)

            logits = model(input_ids, attention_mask)
            loss = bce_full(logits, labels)

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            total_loss += loss.item()
            steps += 1
        avg_loss = total_loss / max(steps, 1)
        print(f"[FULL] Epoch {epoch}/{EPOCHS} - loss: {avg_loss:.4f}")


    # ìµœì¢… ëª¨ë¸ ì €ì¥ ê²½ë¡œëŠ” "í˜„ì¬ ì‘ì—… ë””ë ‰í„°ë¦¬" ê¸°ì¤€
    torch.save(
        {
            "model_state_dict": model.state_dict(),
            "label_list": label_list,
            "kw2cat": kw2cat,
            "config": {
                "BASE_MODEL_NAME": BASE_MODEL_NAME,
                "MAX_SEQ_LEN": MAX_SEQ_LEN,
            },
        },
        CHECKPOINT_PATH,
    )
    print(f"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ â†’ {CHECKPOINT_PATH}")


# ============================================================
# 11. ë¦¬ë·° ì»¨í…ìŠ¤íŠ¸(bread/beverage/generic) íŒë³„
# ============================================================

BREAD_WORDS = [
    "ë¹µ", "í¬ë¡œì™€ìƒ", "í¬ë£¨ì•„ìƒ", "ë„ë„›", "ë„ë„ˆì¸ ",
    "ê½ˆë°°ê¸°", "í¬ë¡œí”Œ", "ìŠ¤ì½˜", "íƒ€ë¥´íŠ¸", "ë§ˆì¹´ë¡±",
    "ë§ˆë“¤ë Œ", "íœ˜ë‚­ì‹œì—", "ê¹ŒëˆŒë ˆ", "ë² ì´ê¸€",
    "ì¼€ì´í¬", "ë¡¤ì¼€ì´í¬", "ë¸Œë¼ìš°ë‹ˆ", "íŒŒì´",
    "ì‹ë¹µ", "ë°”ê²ŒíŠ¸", "í˜¸ë°€ë¹µ", "ì¿ í‚¤",
]

# ë¹µ/ìŒë£Œ ì¬ë£Œí˜• í‚¤ì›Œë“œ (ê³¼ì¼/ê²¬ê³¼ ë“±)
INGREDIENT_LIKE_KEYWORDS = {
    "ê³ êµ¬ë§ˆ", "ë‹¨í˜¸ë°•", "ë°¤", "í”¼ì¹¸", "í”¼ìŠ¤íƒ€ì¹˜ì˜¤", "í—¤ì´ì¦ë„›",
    "ë”¸ê¸°", "ë§ê³ ", "ë ˆëª¬", "ê·¤", "ìëª½", "ì˜¤ë Œì§€", "í¬ë„",
    "ë¸”ë£¨ë² ë¦¬", "ì²´ë¦¬", "ë¬´í™”ê³¼", "ë°”ë‚˜ë‚˜",
}


def classify_review_context(
    text: str,
    place_profile: Dict[str, Any],
) -> str:
    norm = normalize_text_for_rule(text)
    has_bread = any(w in norm for w in BREAD_WORDS)
    has_bev = any(w in norm for w in BEVERAGE_TOKENS)

    if has_bread:
        return "bread"
    if has_bev:
        return "beverage"
    return "generic"


# ============================================================
# 12. ì¶”ë¡ ìš©: ëª¨ë¸ ë¡œë“œ & ì˜ˆì¸¡
# ============================================================

def load_trained_model_for_inference():
    """
    DB ê¸°ë°˜ ì¶”ë¡ ì—ì„œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜.
    ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œëŠ” model_ver11.pyê°€ ìˆëŠ” ë””ë ‰í„°ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ì°¾ëŠ”ë‹¤.
    """
    model_dir = os.path.dirname(__file__)
    ckpt_path = os.path.join(model_dir, CHECKPOINT_PATH)

    print(f"ğŸ” CKPT PATH: {ckpt_path}")

    if not os.path.exists(ckpt_path):
        raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ckpt_path}")

    ckpt = torch.load(ckpt_path, map_location=DEVICE)
    label_list = ckpt["label_list"]
    kw2cat = ckpt["kw2cat"]
    cfg = ckpt["config"]

    model = KeywordExtractorModel(num_labels=len(label_list))
    model.load_state_dict(ckpt["model_state_dict"])
    model.to(DEVICE)
    model.eval()

    tokenizer = AutoTokenizer.from_pretrained(cfg["BASE_MODEL_NAME"])
    return model, tokenizer, label_list, kw2cat


def predict_keywords_for_texts(
    texts: List[str],
    model: KeywordExtractorModel,
    tokenizer: AutoTokenizer,
    label_list: List[str],
    kw_by_cat: Dict[str, List[str]],
    kw2cat: Dict[str, str],
    place_profile: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """
    - ë¦¬ë·° ë¦¬ìŠ¤íŠ¸(texts)ë¥¼ ë°›ì•„ í‚¤ì›Œë“œ ë¶„ë¥˜ + rule-based ë³´ì™„ + ë¶€ì • ë¬¸ë§¥ ì œê±°
    - ê° í‚¤ì›Œë“œë³„ 'ì–‘ì„± ë¦¬ë·° ìˆ˜(pos_count)'ì™€ 'ë¹„ìœ¨(ratio)'ê¹Œì§€ í•¨ê»˜ ë°˜í™˜
    """
    if not texts:
        return {
            "menu_labels": [],
            "topping_labels": [],
            "taste_labels": [],
            "texture_labels": [],
            "store_labels": [],
            "final_keywords": [],
            "keyword_stats": {},
        }

    if place_profile is None:
        place_profile = {
            "is_beverage_shop": False,
            "menu_keywords": set(),
            "beverage_score": 0,
            "dessert_score": 0,
        }

    is_beverage_shop = place_profile.get("is_beverage_shop", False)

    idx_by_cat = {"menu": [], "topping": [], "taste": [], "texture": [], "store": []}
    for i, kw in enumerate(label_list):
        cat = kw2cat.get(kw, "menu")
        if cat in idx_by_cat:
            idx_by_cat[cat].append(i)

    # í‚¤ì›Œë“œê°€ "ë¹µ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë“±ì¥í–ˆëŠ”ì§€" ê¸°ë¡
    kw_seen_in_bread_ctx: Dict[str, bool] = {kw: False for kw in label_list}
    # í‚¤ì›Œë“œê°€ "ê¸ì •/ì¤‘ë¦½ ë¬¸ë§¥"ìœ¼ë¡œ ë“±ì¥í•œ ë¦¬ë·° ìˆ˜
    kw_positive_counts: Dict[str, int] = {kw: 0 for kw in label_list}

    all_probs = []
    for text in texts:
        ctx = classify_review_context(text, place_profile)
        norm = normalize_text_for_rule(text)

        # ê¸ì • ë¬¸ë§¥ ë“±ì¥ ì¹´ìš´íŠ¸
        for kw in label_list:
            if has_positive_occurrence(kw, norm):
                kw_positive_counts[kw] += 1

        enc = tokenizer(
            text,
            padding="max_length",
            truncation=True,
            max_length=MAX_SEQ_LEN,
            return_tensors="pt",
        )

        with torch.no_grad():
            logits = model(
                input_ids=enc["input_ids"].to(DEVICE),
                attention_mask=enc["attention_mask"].to(DEVICE),
            )
            probs = torch.sigmoid(logits).cpu().numpy()[0]

        # ìŒë£Œ ì»¨í…ìŠ¤íŠ¸ì—ì„œëŠ” taste/topping ì°¨ë‹¨
        if ctx == "beverage" or (ctx == "generic" and is_beverage_shop):
            for idx in idx_by_cat["topping"]:
                probs[idx] = 0.0
            for idx in idx_by_cat["taste"]:
                probs[idx] = 0.0

        # ë¹µ ì»¨í…ìŠ¤íŠ¸(ë˜ëŠ” ë¹µ ìœ„ì£¼ generic)ì—ì„œëŠ”
        # ì‹¤ì œ ë“±ì¥í•œ í‚¤ì›Œë“œë¥¼ ëª¨ë‘ "ë¹µ ë¬¸ë§¥ì—ì„œ ë³¸ ê²ƒ"ìœ¼ë¡œ ê¸°ë¡
        if (ctx == "bread") or (ctx == "generic" and not is_beverage_shop):
            for kw in label_list:
                kw_norm = kw.replace(" ", "").lower()
                if kw_norm and kw_norm in norm:
                    kw_seen_in_bread_ctx[kw] = True

        all_probs.append(probs)

    avg_probs = np.mean(all_probs, axis=0)
    kw_scores = {kw: avg_probs[i] for i, kw in enumerate(label_list)}

    base_thresh_menu = 0.5
    base_thresh_oth = 0.4

    menu_labels, topping_labels, taste_labels = [], [], []
    texture_labels, store_labels = [], []

    for kw, score in kw_scores.items():
        cat = kw2cat.get(kw, "menu")
        if cat == "menu":
            if score >= base_thresh_menu:
                menu_labels.append(kw)
        elif cat == "topping":
            if score >= base_thresh_oth:
                topping_labels.append(kw)
        elif cat == "taste":
            if score >= base_thresh_oth:
                taste_labels.append(kw)
        elif cat == "texture":
            if score >= base_thresh_oth:
                texture_labels.append(kw)
        elif cat == "store":
            if score >= base_thresh_oth:
                store_labels.append(kw)

    # ë¦¬ë·° ì „ì²´ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
    concat_text = "\n".join(texts)
    concat_norm = normalize_text_for_rule(concat_text)

    # rule-based (ì „ì²´ ë¦¬ë·° ê¸°ì¤€, ë¶€ì • ë¬¸ë§¥ ì œê±° í¬í•¨)
    rb_hits = rule_based_extract_keywords(concat_text, kw_by_cat)
    for k in rb_hits.get("menu", []):
        if k not in menu_labels and k in label_list:
            menu_labels.append(k)
    for k in rb_hits.get("topping", []):
        if k not in topping_labels and k in label_list:
            topping_labels.append(k)
    for k in rb_hits.get("taste", []):
        if k not in taste_labels and k in label_list:
            taste_labels.append(k)
    for k in rb_hits.get("texture", []):
        if k not in texture_labels and k in label_list:
            texture_labels.append(k)
    for k in rb_hits.get("store", []):
        if k not in store_labels and k in label_list:
            store_labels.append(k)

    # taste/toppingì€ "ë¹µ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ ë“±ì¥" + "ê¸ì • ë¬¸ë§¥ ë“±ì¥" ë‘˜ ë‹¤ í•„ìš”
    def filter_taste_topping(kws: List[str]) -> List[str]:
        kept = []
        for k in kws:
            if not kw_seen_in_bread_ctx.get(k, False):
                continue
            if kw_positive_counts.get(k, 0) <= 0:
                continue
            kept.append(k)
        return kept

    topping_labels = filter_taste_topping(topping_labels)
    taste_labels = filter_taste_topping(taste_labels)

    # menu/store/texture ë„ "ê¸ì • ë¬¸ë§¥ìœ¼ë¡œ ë“±ì¥í•œ ì ì´ ìˆëŠ” í‚¤ì›Œë“œ"ë§Œ ìœ ì§€
    def filter_by_positive_count(kws: List[str]) -> List[str]:
        return [k for k in kws if kw_positive_counts.get(k, 0) > 0]

    menu_labels = filter_by_positive_count(menu_labels)
    store_labels = filter_by_positive_count(store_labels)
    texture_labels = filter_by_positive_count(texture_labels)

    # ìŒë£Œ ìœ„ì£¼ ì¹´í˜ì—ì„œëŠ” ì¬ë£Œí˜• í‚¤ì›Œë“œ(INGREDIENT_LIKE_KEYWORDS)ê°€
    # ë¹µ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë“±ì¥í•˜ì§€ ì•Šì•˜ë‹¤ë©´ ì œê±°
    if is_beverage_shop:
        filtered_menu = []
        for k in menu_labels:
            if k in INGREDIENT_LIKE_KEYWORDS and not kw_seen_in_bread_ctx.get(k, False):
                continue
            filtered_menu.append(k)
        menu_labels = filtered_menu

    menu_labels = sorted(set(menu_labels))
    topping_labels = sorted(set(topping_labels))
    taste_labels = sorted(set(taste_labels))
    texture_labels = sorted(set(texture_labels))
    store_labels = sorted(set(store_labels))

    final_keywords = sorted(
        set(menu_labels + topping_labels + taste_labels + texture_labels + store_labels)
    )

    # --------------------------------------------------------
    # ê° í‚¤ì›Œë“œë³„ 'ì–‘ì„± ë¦¬ë·° ìˆ˜' / 'ë¹„ìœ¨' ê³„ì‚°
    # --------------------------------------------------------
    num_reviews = len(texts)
    kw_ratio = {
        kw: (kw_positive_counts[kw] / num_reviews) if num_reviews > 0 else 0.0
        for kw in label_list
    }

    keyword_stats = {
        kw: {
            "pos_count": int(kw_positive_counts[kw]),
            "ratio": float(kw_ratio[kw]),
        }
        for kw in final_keywords
    }

    return {
        "menu_labels": menu_labels,
        "topping_labels": topping_labels,
        "taste_labels": taste_labels,
        "texture_labels": texture_labels,
        "store_labels": store_labels,
        "final_keywords": final_keywords,
        # ë‚˜ì¤‘ì— "ì—ê·¸íƒ€ë¥´íŠ¸ ë§›ì§‘ ì¬ì •ë ¬" ë“±ì— í™œìš©í•  ìˆ˜ ìˆëŠ” ì •ë³´
        "keyword_stats": keyword_stats,
    }


def run_predict_folder(test_dir: str = "test"):
    """
    test_dir ì•ˆì˜ JSON(ë§¤ì¥ ë‹¨ìœ„)ì„ ì½ì–´ì„œ
    - reviews â†’ í‚¤ì›Œë“œ ì˜ˆì¸¡
    - data["keywords"]ë¥¼ ë®ì–´ì“°ê³  ë‹¤ì‹œ ì €ì¥
    """
    # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë¥¼ model_ver11.py ê¸°ì¤€ìœ¼ë¡œ ë³´ì •
    model_dir = os.path.dirname(__file__)
    ckpt_path = os.path.join(model_dir, CHECKPOINT_PATH)

    if not os.path.exists(ckpt_path):
        print(f"âŒ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤: {ckpt_path}")
        return

    base_kw_path = os.path.join(model_dir, BASE_KEYWORD_PATH)
    new_kw_path = os.path.join(model_dir, NEW_KEYWORD_PATH)
    dessert_meta_path = os.path.join(model_dir, DESSERT_META_PATH)

    kw_by_cat, label_list_all, label2id, id2label, kw2cat = load_keyword_config(
        base_kw_path, new_kw_path
    )
    place_profiles = load_dessert_profiles(dessert_meta_path)
    model, tokenizer, label_list_ckpt, kw2cat_ckpt = load_trained_model_for_inference()

    if label_list_ckpt != label_list_all:
        print("âš ï¸ ì£¼ì˜: ì²´í¬í¬ì¸íŠ¸ì˜ label_listì™€ í˜„ì¬ í‚¤ì›Œë“œ êµ¬ì„±ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    files = [f for f in os.listdir(test_dir) if f.endswith(".json")]
    print(f"ğŸ“‚ í…ŒìŠ¤íŠ¸ íŒŒì¼ ê°œìˆ˜: {len(files)}")

    for fname in files:
        path = os.path.join(test_dir, fname)
        data = load_json(path)
        place = data.get("place_name", fname)

        profile = get_place_profile(place, place_profiles)
        reviews = data.get("reviews", [])
        texts = [r.get("review_content", "") for r in reviews if r.get("review_content", "")]

        print("=" * 60)
        print(f"ğŸ·  {fname} | ë§¤ì¥ëª…: {place} | ë¦¬ë·° {len(texts)}ê°œ")

        kw_result = predict_keywords_for_texts(
            texts, model, tokenizer, label_list_ckpt, kw_by_cat, kw2cat_ckpt, place_profile=profile
        )
        data["keywords"] = kw_result
        save_json(data, path)
        print(f"  âœ… ì˜ˆì¸¡ëœ í‚¤ì›Œë“œ: {kw_result['final_keywords']}")
        print(f"  ğŸ’¾ ì €ì¥ ì™„ë£Œ â†’ {path}")


# ============================================================
# 13. main (JSON í•™ìŠµ/í…ŒìŠ¤íŠ¸ìš©, DB ì¶”ë¡ ê³¼ëŠ” ë³„ê°œ)
# ============================================================

def main():
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "mode",
        choices=["train", "predict-folder"],
        help="train: ëª¨ë¸ í•™ìŠµ / predict-folder: í´ë” ë‚´ JSON ì˜ˆì¸¡",
    )
    parser.add_argument(
        "--test_dir",
        type=str,
        default="test",
        help="predict-folder ëª¨ë“œì—ì„œ ì‚¬ìš©í•  í´ë” ê²½ë¡œ",
    )
    args = parser.parse_args()

    set_seed(42)
    # model_ver11.py íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í„°ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ë³´ì •
    model_dir = os.path.dirname(__file__)
    base_kw_path = os.path.join(model_dir, BASE_KEYWORD_PATH)
    new_kw_path = os.path.join(model_dir, NEW_KEYWORD_PATH)
    dessert_meta_path = os.path.join(model_dir, DESSERT_META_PATH)
    if args.mode == "train":
        print("============================================================")
        print("ğŸ KoELECTRA í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ver11)")
        print("============================================================")
        kw_by_cat, label_list, label2id, id2label, kw2cat = load_keyword_config(
            base_kw_path, new_kw_path
        )
        place_profiles = load_dessert_profiles(dessert_meta_path)
        samples = load_train_data(TRAIN_DIR, label2id, kw_by_cat, place_profiles)
        run_kfold_training(samples, label_list, kw2cat)

    elif args.mode == "predict-folder":
        run_predict_folder(args.test_dir)


if __name__ == "__main__":
    main()
